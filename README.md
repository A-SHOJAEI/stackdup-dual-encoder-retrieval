# stackdup-dual-encoder-retrieval

Duplicate-question detection framed as retrieval: given a query Stack Overflow question, retrieve its canonical duplicate(s) from an indexed corpus using (1) sparse baselines and (2) a trained dual-encoder (bi-encoder) with optional hard-negative mining.

This repo is organized around two pipelines:

1. **Smoke pipeline (what the current results are from):** a synthetic JSONL retrieval dataset + TF-IDF/BM25 baselines + two bi-encoder training runs (with vs without mined hard negatives), producing `artifacts/results.json` and `artifacts/report.md`.
2. **Full Stack Overflow data dump pipeline (implemented, not executed by default):** download + verify + extract the Stack Exchange dump, stream-parse `Posts.xml` and `PostLinks.xml` into SQLite, then time-split into the same JSONL format consumed by baselines and the bi-encoder.

## Dataset Provenance

### Smoke dataset (synthetic; used in `artifacts/results.json`)

Generated by `src/stackdup/data/synthetic.py` via:

```bash
.venv/bin/python -m stackdup.data.synthetic --config configs/smoke_data.yaml
```

Configuration (`configs/smoke_data.yaml`):
- `n_docs: 300`, `n_queries: 120`, `vocab_size: 200`, `text_style: title_body`
- Each query is constructed by perturbing the text of exactly one randomly chosen target document and writing a single qrel `(query_id -> doc_id)`.

This design makes sparse lexical baselines extremely strong because the query text is a lightly edited subset of its relevant document's tokens.

### Stack Overflow dataset (real; pipeline implemented)

Download URLs are explicitly listed in `configs/stackoverflow.yaml` and hardcoded in `src/stackdup/data/download.py`:
- `stackoverflow.com-Posts.7z`
- `stackoverflow.com-PostLinks.7z`
- `readme.txt`

Provenance and verification:
- Files are fetched from Archive.org (`https://archive.org/download/stackexchange/...`).
- `src/stackdup/data/archiveorg.py` queries `https://archive.org/metadata/stackexchange` and verifies **file size** and (when present in metadata) **MD5**.

Label source:
- `src/stackdup/data/build_pairs.py` stream-parses:
  - `Posts.xml`: questions only (`PostTypeId=1`)
  - `PostLinks.xml`: duplicate edges only (`LinkTypeId=3`)
- It writes `data/processed/stackoverflow.sqlite` with:
  - `questions(id, creation_date, title, body, text)`
  - `dup_links(src_id, tgt_id)` (directed edges as represented in the dump)

Text construction:
- Title is always included.
- Body is optional; when included, HTML is stripped via BeautifulSoup (`src/stackdup/data/html_clean.py`) with optional removal of `<pre>/<code>` blocks.

Time-based split:
- `src/stackdup/data/split.py` assigns questions to train/val/test by `creation_date` cutoffs.
- Default cutoffs in `configs/stackoverflow.yaml`: `train_end=2018-01-01`, `val_end=2019-01-01`.
- Retrieval files written to `data/splits/`:
  - `corpus.jsonl` (docs from `train + val` by default)
  - `train_pairs.jsonl`, `val_pairs.jsonl` from duplicate edges where `src` is in split and `tgt` is in corpus
  - `test_queries.jsonl` and `test_qrels.jsonl` from edges where `src` is in test and `tgt` is in corpus

Licensing note: Stack Overflow user contributions are CC BY-SA 4.0 (via the Stack Exchange Data Dump).

## Methodology

### Baselines

Implemented baselines (smoke-friendly, no Java):
- **TF-IDF + cosine** (`src/stackdup/baselines/tfidf_eval.py`): `TfidfVectorizer(ngram_range=(1,2), max_features=200000)` + cosine similarity.
- **BM25 (pure Python)** (`src/stackdup/baselines/bm25_eval.py`): `rank_bm25.BM25Okapi` with whitespace tokenization.

Optional plan-grade baseline:
- **Lucene BM25 via Pyserini** (`src/stackdup/baselines/pyserini_index.py`, `src/stackdup/baselines/pyserini_eval.py`), requires Java and `requirements-pyserini.txt`.

### Dual-Encoder Retrieval Model

Encoder (`src/stackdup/modeling/encoder.py`):
- Transformer backbone from Hugging Face (`AutoModel.from_pretrained`)
- Pooling: `mean` or `cls` over last hidden states
- Optional L2 normalization (enabled in smoke configs), enabling dot-product = cosine similarity

Training objective (`src/stackdup/train/biencoder.py`):
- **In-batch InfoNCE**: for a batch of size `B`, compute `logits = (Q @ P^T) / temperature` and apply cross-entropy with labels `0..B-1`.
- Optional **mined hard-negative loss**: for each query, append up to `K` mined negatives and apply cross-entropy over `[pos + negatives]`.
- Total loss: `loss = loss_inbatch + mined_neg_weight * loss_mined`.

Hard-negative mining (`src/stackdup/mining/mine_negatives.py`):
- `mode=bm25`: mine negatives using BM25 over the corpus.
- `mode=ann`: mine negatives by embedding corpus and querying an ANN index (`faiss.IndexFlatIP`) or bruteforce fallback.

Smoke baseline config details (from `configs/smoke_baseline.yaml` / `configs/smoke_no_hardneg.yaml`):
- Backbone: `sshleifer/tiny-distilbert-base-cased`
- `max_length=96`, `pooling=mean`, `normalize=true`
- Baseline run enables BM25-mined negatives with `k=8` and refreshes mining once before training.
- Ablation disables mining entirely (in-batch negatives only).

### Evaluation and Metrics

Bi-encoder evaluation (`src/stackdup/eval/retrieval.py`):
- Encode full corpus and queries, then rank by inner product using FAISS (`IndexFlatIP`) when available (falls back to bruteforce).

Metrics (`src/stackdup/utils/metrics.py`):
- `recall@k`: binary hit if any relevant doc appears in top-k
- `mrr@10`
- `ndcg@10` with binary relevance
- `n_eval`: number of queries with at least one qrel

## Results (Smoke)

The authoritative report is `artifacts/report.md` generated by `src/stackdup/reporting/make_report.py` from `artifacts/results.json`.
For an exact reference: the table lives under `# Retrieval Results` in `artifacts/report.md`, and the deltas live under `## Comparisons`.

Reported metrics table (copied from `artifacts/report.md`):

| name | kind | recall@1 | recall@5 | recall@10 | recall@50 | mrr@10 | ndcg@10 | n_eval |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| bm25_python | baseline | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 120.0000 |
| tfidf_cosine | baseline | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 120.0000 |
| smoke_biencoder_baseline | biencoder | 0.0000 | 0.0167 | 0.0500 | 0.1583 | 0.0101 | 0.0190 | 120.0000 |
| smoke_biencoder_no_hardneg | biencoder | 0.0083 | 0.0167 | 0.0333 | 0.1917 | 0.0131 | 0.0177 | 120.0000 |

Comparison block in `artifacts/results.json` / `artifacts/report.md`:
- `biencoder_baseline_minus_no_hardneg` deltas:
  - `mrr@10 = -0.0030555556`
  - `ndcg@10 = 0.0013587881`
  - `recall@1 = -0.0083333333`
  - `recall@10 = 0.0166666667`
  - `recall@50 = -0.0333333333`

Interpretation constraint: these numbers are from the synthetic smoke dataset where lexical overlap is intentionally high, which is consistent with TF-IDF/BM25 scoring perfectly.

## Reproducibility / How To Reproduce

### One-command smoke reproduction

```bash
make all
```

This runs (see `Makefile`). Minimum requirements:
- `python3` (creates `.venv` and installs deps)
- For the full dump pipeline only: a `7z` extractor (`7zz` or `7z`)
- For the optional Lucene BM25 baseline only: Java + `requirements-pyserini.txt`

Pipeline steps:
1. `make setup`: creates `.venv` and installs `requirements.txt` + editable package
2. `make data`: generates `data/smoke/*.jsonl`
3. `make train`: trains both bi-encoder configs into `runs/*/`
4. `make eval`: writes per-experiment JSON into `artifacts/*.json` and aggregates into `artifacts/results.json`
5. `make report`: renders `artifacts/report.md`

### Running individual steps

```bash
make setup
make data
make train CONFIG_BASELINE=configs/smoke_baseline.yaml CONFIG_ABLATION=configs/smoke_no_hardneg.yaml
make eval
make report
```

### Full Stack Overflow dump pipeline (implemented)

```bash
make setup

.venv/bin/python -m stackdup.data.download --dataset stackoverflow --out data/raw
.venv/bin/python -m stackdup.data.extract --in-dir data/raw --out-dir data/extracted

.venv/bin/python -m stackdup.data.build_pairs \
  --posts data/extracted/Posts.xml \
  --links data/extracted/PostLinks.xml \
  --out data/processed \
  --include-body \
  --strip-code-blocks \
  --max-body-chars 20000

.venv/bin/python -m stackdup.data.split \
  --db data/processed/stackoverflow.sqlite \
  --out data/splits \
  --train-end 2018-01-01 \
  --val-end 2019-01-01
```

Once `data/splits/` exists, point the baselines / training configs' `data.*` paths at those JSONLs and run the same CLIs used in smoke (`stackdup.baselines.*`, `stackdup.train.biencoder`, `stackdup.eval.retrieval`).

## Limitations

- The checked-in results are **smoke-only** and come from a synthetic generator where queries are derived from the relevant document text; sparse baselines saturate (1.0) and are not representative of the real Stack Overflow task.
- `train/biencoder.py`'s `refresh_before_training` only supports `mode=bm25` (ANN refresh is available via the mining CLI, not auto-refresh).
- No re-ranking stage (e.g., cross-encoder) is implemented; this repo is retrieval-only.
- Metrics are binary relevance; `ndcg@10` does not model graded relevance.
- For the real dump, duplicate links are used as directed edges exactly as in `PostLinks.xml`; downstream evaluation may benefit from canonicalization or symmetrization depending on the application.

## Next Research Steps (Concrete)

1. Run the full dump pipeline on a real subset and report metrics for TF-IDF, BM25 (Lucene via Pyserini), and the bi-encoder on that data.
2. Replace one-shot BM25 mining with iterative **bi-encoder ANN mining** (FAISS) refreshed every N steps/epochs; consider mixing BM25 and ANN negatives.
3. Use a stronger backbone and training recipe (more steps, larger batches, gradient accumulation, better temperature tuning), and add multi-positive handling for queries with multiple duplicates.
4. Add a second-stage re-ranker (cross-encoder) and evaluate end-to-end recall/latency tradeoffs.
5. Improve text normalization (tag removal, code handling, truncation strategy) and measure sensitivity (title-only vs title+body, code kept vs dropped).

## License

- Code: MIT (`LICENSE`)
- Data: Stack Overflow / Stack Exchange Data Dump, CC BY-SA 4.0
