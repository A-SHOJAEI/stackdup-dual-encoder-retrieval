
[2026-02-10 08:11:24 UTC] autofix: patched Makefile setup for venv bootstrap/PEP668

[2026-02-10 08:11:24 UTC] $ make setup
Collecting pip
  Using cached pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)
Using cached pip-26.0.1-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
Successfully installed pip-26.0.1
Collecting pip==24.2
  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)
Collecting setuptools==75.6.0
  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)
Collecting wheel==0.43.0
  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)
Using cached pip-24.2-py3-none-any.whl (1.8 MB)
Downloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 12.6 MB/s  0:00:00
Using cached wheel-0.43.0-py3-none-any.whl (65 kB)
Installing collected packages: wheel, setuptools, pip
  Attempting uninstall: pip
    Found existing installation: pip 26.0.1
    Uninstalling pip-26.0.1:
      Successfully uninstalled pip-26.0.1

Successfully installed pip-24.2 setuptools-75.6.0 wheel-0.43.0

[notice] A new release of pip is available: 24.2 -> 26.0.1
[notice] To update, run: /home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/bin/python -m pip install --upgrade pip
Collecting numpy==1.26.4 (from -r requirements.txt (line 2))
  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting scipy==1.12.0 (from -r requirements.txt (line 3))
  Downloading scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)
Collecting tqdm==4.66.4 (from -r requirements.txt (line 4))
  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)
Collecting pyyaml==6.0.2 (from -r requirements.txt (line 5))
  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting requests==2.32.3 (from -r requirements.txt (line 6))
  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting lxml==5.2.2 (from -r requirements.txt (line 9))
  Using cached lxml-5.2.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.4 kB)
Collecting beautifulsoup4==4.12.3 (from -r requirements.txt (line 10))
  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)
Collecting scikit-learn==1.5.1 (from -r requirements.txt (line 13))
  Using cached scikit_learn-1.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting rank-bm25==0.2.2 (from -r requirements.txt (line 14))
  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)
Collecting torch==2.4.1 (from -r requirements.txt (line 17))
  Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)
Collecting transformers==4.44.2 (from -r requirements.txt (line 18))
  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)
Collecting accelerate==0.33.0 (from -r requirements.txt (line 19))
  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)
Collecting sentencepiece==0.2.0 (from -r requirements.txt (line 20))
  Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting faiss-cpu==1.8.0.post1 (from -r requirements.txt (line 23))
  Using cached faiss_cpu-1.8.0.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)
Collecting charset-normalizer<4,>=2 (from requests==2.32.3->-r requirements.txt (line 6))
  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)
Collecting idna<4,>=2.5 (from requests==2.32.3->-r requirements.txt (line 6))
  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)
Collecting urllib3<3,>=1.21.1 (from requests==2.32.3->-r requirements.txt (line 6))
  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)
Collecting certifi>=2017.4.17 (from requests==2.32.3->-r requirements.txt (line 6))
  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)
Collecting soupsieve>1.2 (from beautifulsoup4==4.12.3->-r requirements.txt (line 10))
  Downloading soupsieve-2.8.3-py3-none-any.whl.metadata (4.6 kB)
Collecting joblib>=1.2.0 (from scikit-learn==1.5.1->-r requirements.txt (line 13))
  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)
Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.5.1->-r requirements.txt (line 13))
  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)
Collecting filelock (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)
Collecting typing-extensions>=4.8.0 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting sympy (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)
Collecting jinja2 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting fsspec (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (75.6.0)
Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.0.0 (from torch==2.4.1->-r requirements.txt (line 17))
  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)
Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.44.2->-r requirements.txt (line 18))
  Using cached huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)
Collecting packaging>=20.0 (from transformers==4.44.2->-r requirements.txt (line 18))
  Using cached packaging-26.0-py3-none-any.whl.metadata (3.3 kB)
Collecting regex!=2019.12.17 (from transformers==4.44.2->-r requirements.txt (line 18))
  Using cached regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)
Collecting safetensors>=0.4.1 (from transformers==4.44.2->-r requirements.txt (line 18))
  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2->-r requirements.txt (line 18))
  Using cached tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting psutil (from accelerate==0.33.0->-r requirements.txt (line 19))
  Using cached psutil-7.2.2-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)
Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->-r requirements.txt (line 17))
  Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->-r requirements.txt (line 18))
  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.1->-r requirements.txt (line 17))
  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.1->-r requirements.txt (line 17))
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)
Downloading scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37.8/37.8 MB 31.8 MB/s eta 0:00:00
Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)
Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)
Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Using cached lxml-5.2.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)
Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)
Using cached scikit_learn-1.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)
Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)
Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)
Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)
Using cached accelerate-0.33.0-py3-none-any.whl (315 kB)
Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Using cached faiss_cpu-1.8.0.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)
Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)
Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)
Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)
Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)
Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)
Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)
Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)
Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)
Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)
Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)
Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)
Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)
Using cached huggingface_hub-0.36.2-py3-none-any.whl (566 kB)
Using cached fsspec-2026.2.0-py3-none-any.whl (202 kB)
Using cached idna-3.11-py3-none-any.whl (71 kB)
Using cached joblib-1.5.3-py3-none-any.whl (309 kB)
Using cached packaging-26.0-py3-none-any.whl (74 kB)
Using cached regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)
Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)
Downloading soupsieve-2.8.3-py3-none-any.whl (37 kB)
Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)
Using cached tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)
Using cached filelock-3.20.3-py3-none-any.whl (16 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)
Using cached psutil-7.2.2-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (155 kB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)
Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)
Installing collected packages: sentencepiece, mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, soupsieve, safetensors, regex, pyyaml, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, lxml, joblib, idna, hf-xet, fsspec, filelock, charset-normalizer, certifi, triton, scipy, requests, rank-bm25, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, faiss-cpu, beautifulsoup4, scikit-learn, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate
Successfully installed MarkupSafe-3.0.3 accelerate-0.33.0 beautifulsoup4-4.12.3 certifi-2026.1.4 charset-normalizer-3.4.4 faiss-cpu-1.8.0.post1 filelock-3.20.3 fsspec-2026.2.0 hf-xet-1.2.0 huggingface-hub-0.36.2 idna-3.11 jinja2-3.1.6 joblib-1.5.3 lxml-5.2.2 mpmath-1.3.0 networkx-3.6.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 packaging-26.0 psutil-7.2.2 pyyaml-6.0.2 rank-bm25-0.2.2 regex-2026.1.15 requests-2.32.3 safetensors-0.7.0 scikit-learn-1.5.1 scipy-1.12.0 sentencepiece-0.2.0 soupsieve-2.8.3 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.19.1 torch-2.4.1 tqdm-4.66.4 transformers-4.44.2 triton-3.0.0 typing-extensions-4.15.0 urllib3-2.6.3

[notice] A new release of pip is available: 24.2 -> 26.0.1
[notice] To update, run: /home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/bin/python -m pip install --upgrade pip
Obtaining file:///home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Building wheels for collected packages: stackdup-dual-encoder-retrieval
  Building editable for stackdup-dual-encoder-retrieval (pyproject.toml): started
  Building editable for stackdup-dual-encoder-retrieval (pyproject.toml): finished with status 'done'
  Created wheel for stackdup-dual-encoder-retrieval: filename=stackdup_dual_encoder_retrieval-0.1.0-0.editable-py3-none-any.whl size=4504 sha256=0a3936fbbc949c1595262b41679dea98c3e6d6e5e6536ccdd9db74c7fc8e2f71
  Stored in directory: /tmp/pip-ephem-wheel-cache-37xed2k7/wheels/2e/53/93/2e18ee5002870e3538d9fd4ee5e052cf27acb0fb944d7fdbce
Successfully built stackdup-dual-encoder-retrieval
Installing collected packages: stackdup-dual-encoder-retrieval
Successfully installed stackdup-dual-encoder-retrieval-0.1.0

[notice] A new release of pip is available: 24.2 -> 26.0.1
[notice] To update, run: /home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/bin/python -m pip install --upgrade pip
Venv ready: .venv

[2026-02-10 08:12:20 UTC] $ make all
Requirement already satisfied: pip==24.2 in ./.venv/lib/python3.12/site-packages (24.2)
Requirement already satisfied: setuptools==75.6.0 in ./.venv/lib/python3.12/site-packages (75.6.0)
Requirement already satisfied: wheel==0.43.0 in ./.venv/lib/python3.12/site-packages (0.43.0)

[notice] A new release of pip is available: 24.2 -> 26.0.1
[notice] To update, run: /home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/bin/python -m pip install --upgrade pip
Requirement already satisfied: numpy==1.26.4 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.26.4)
Requirement already satisfied: scipy==1.12.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.12.0)
Requirement already satisfied: tqdm==4.66.4 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (4.66.4)
Requirement already satisfied: pyyaml==6.0.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (6.0.2)
Requirement already satisfied: requests==2.32.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.32.3)
Requirement already satisfied: lxml==5.2.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (5.2.2)
Requirement already satisfied: beautifulsoup4==4.12.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (4.12.3)
Requirement already satisfied: scikit-learn==1.5.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (1.5.1)
Requirement already satisfied: rank-bm25==0.2.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (0.2.2)
Requirement already satisfied: torch==2.4.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (2.4.1)
Requirement already satisfied: transformers==4.44.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (4.44.2)
Requirement already satisfied: accelerate==0.33.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (0.33.0)
Requirement already satisfied: sentencepiece==0.2.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (0.2.0)
Requirement already satisfied: faiss-cpu==1.8.0.post1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (1.8.0.post1)
Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests==2.32.3->-r requirements.txt (line 6)) (3.4.4)
Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests==2.32.3->-r requirements.txt (line 6)) (3.11)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests==2.32.3->-r requirements.txt (line 6)) (2.6.3)
Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests==2.32.3->-r requirements.txt (line 6)) (2026.1.4)
Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4==4.12.3->-r requirements.txt (line 10)) (2.8.3)
Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 13)) (1.5.3)
Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 13)) (3.6.0)
Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (3.20.3)
Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (4.15.0)
Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (1.14.0)
Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (3.6.1)
Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (3.1.6)
Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (2026.2.0)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (75.6.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (2.20.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (12.1.105)
Requirement already satisfied: triton==3.0.0 in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 17)) (3.0.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.venv/lib/python3.12/site-packages (from transformers==4.44.2->-r requirements.txt (line 18)) (0.36.2)
Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers==4.44.2->-r requirements.txt (line 18)) (26.0)
Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.44.2->-r requirements.txt (line 18)) (2026.1.15)
Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers==4.44.2->-r requirements.txt (line 18)) (0.7.0)
Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.12/site-packages (from transformers==4.44.2->-r requirements.txt (line 18)) (0.19.1)
Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from accelerate==0.33.0->-r requirements.txt (line 19)) (7.2.2)
Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->-r requirements.txt (line 17)) (12.9.86)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->-r requirements.txt (line 18)) (1.2.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.4.1->-r requirements.txt (line 17)) (3.0.3)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch==2.4.1->-r requirements.txt (line 17)) (1.3.0)

[notice] A new release of pip is available: 24.2 -> 26.0.1
[notice] To update, run: /home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/bin/python -m pip install --upgrade pip
Obtaining file:///home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Building wheels for collected packages: stackdup-dual-encoder-retrieval
  Building editable for stackdup-dual-encoder-retrieval (pyproject.toml): started
  Building editable for stackdup-dual-encoder-retrieval (pyproject.toml): finished with status 'done'
  Created wheel for stackdup-dual-encoder-retrieval: filename=stackdup_dual_encoder_retrieval-0.1.0-0.editable-py3-none-any.whl size=4504 sha256=2ab24bfdd6079c8384ef46348b81775d99573e5c7d86c33e413615a64556ead3
  Stored in directory: /tmp/pip-ephem-wheel-cache-6iwy8dns/wheels/2e/53/93/2e18ee5002870e3538d9fd4ee5e052cf27acb0fb944d7fdbce
Successfully built stackdup-dual-encoder-retrieval
Installing collected packages: stackdup-dual-encoder-retrieval
  Attempting uninstall: stackdup-dual-encoder-retrieval
    Found existing installation: stackdup-dual-encoder-retrieval 0.1.0
    Uninstalling stackdup-dual-encoder-retrieval-0.1.0:
      Successfully uninstalled stackdup-dual-encoder-retrieval-0.1.0
Successfully installed stackdup-dual-encoder-retrieval-0.1.0

[notice] A new release of pip is available: 24.2 -> 26.0.1
[notice] To update, run: /home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/bin/python -m pip install --upgrade pip
Venv ready: .venv
Wrote synthetic dataset to data/smoke
  corpus: data/smoke/corpus.jsonl
  train_pairs: data/smoke/train_pairs.jsonl
  val_pairs: data/smoke/val_pairs.jsonl
  test_queries: data/smoke/test_queries.jsonl
  test_qrels: data/smoke/test_qrels.jsonl
BM25 mining:   0%|          | 0/96 [00:00<?, ?ex/s]BM25 mining:  67%|██████▋   | 64/96 [00:00<00:00, 638.83ex/s]BM25 mining: 100%|██████████| 96/96 [00:00<00:00, 634.31ex/s]
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/src/stackdup/train/biencoder.py:198: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=mp)
train epoch 1/1:   0%|          | 0/200 [00:00<?, ?it/s]train epoch 1/1:   0%|          | 0/200 [00:00<?, ?it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   0%|          | 1/200 [00:00<01:06,  2.99it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   0%|          | 1/200 [00:00<01:06,  2.99it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   0%|          | 1/200 [00:00<01:06,  2.99it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   0%|          | 1/200 [00:00<01:06,  2.99it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   2%|▏         | 4/200 [00:00<00:20,  9.67it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   2%|▏         | 4/200 [00:00<00:20,  9.67it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   2%|▏         | 4/200 [00:00<00:20,  9.67it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]train epoch 1/1:   3%|▎         | 6/200 [00:00<00:18, 10.44it/s, loss=4.9698 inbatch=2.7726 mined=2.1972]
Finished training. out_dir=runs/smoke_biencoder_baseline
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/src/stackdup/train/biencoder.py:198: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=mp)
train epoch 1/1:   0%|          | 0/200 [00:00<?, ?it/s]train epoch 1/1:   0%|          | 0/200 [00:00<?, ?it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   0%|          | 1/200 [00:00<00:59,  3.37it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   0%|          | 1/200 [00:00<00:59,  3.37it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   0%|          | 1/200 [00:00<00:59,  3.37it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   0%|          | 1/200 [00:00<00:59,  3.37it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   2%|▏         | 4/200 [00:00<00:16, 12.01it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   2%|▏         | 4/200 [00:00<00:16, 12.01it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   2%|▏         | 4/200 [00:00<00:16, 12.01it/s, loss=2.7726 inbatch=2.7726]train epoch 1/1:   3%|▎         | 6/200 [00:00<00:14, 13.39it/s, loss=2.7726 inbatch=2.7726]
Finished training. out_dir=runs/smoke_biencoder_no_hardneg
Wrote artifacts/tfidf_results.json
Wrote artifacts/bm25_results.json
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/src/stackdup/eval/retrieval.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(str(ckpt_path), map_location="cpu")
encode:   0%|          | 0/5 [00:00<?, ?batch/s]encode:  20%|██        | 1/5 [00:00<00:01,  2.42batch/s]encode: 100%|██████████| 5/5 [00:00<00:00, 11.26batch/s]
encode:   0%|          | 0/2 [00:00<?, ?batch/s]encode: 100%|██████████| 2/2 [00:00<00:00, 147.40batch/s]
Wrote artifacts/biencoder_baseline_results.json
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/home/alireza/research-autopilot/generated_projects/stackdup-dual-encoder-retrieval/src/stackdup/eval/retrieval.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  obj = torch.load(str(ckpt_path), map_location="cpu")
encode:   0%|          | 0/5 [00:00<?, ?batch/s]encode:  20%|██        | 1/5 [00:00<00:01,  2.44batch/s]encode: 100%|██████████| 5/5 [00:00<00:00, 11.32batch/s]
encode:   0%|          | 0/2 [00:00<?, ?batch/s]encode: 100%|██████████| 2/2 [00:00<00:00, 143.07batch/s]
Wrote artifacts/biencoder_ablation_results.json
Wrote artifacts/results.json
Wrote artifacts/report.md
